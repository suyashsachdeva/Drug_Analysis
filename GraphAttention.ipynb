{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "import torch_geometric as thg\n",
    "import torch_geometric.nn as gnn\n",
    "import torch_geometric.nn.functional as gf\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils.convert import from_scipy_sparse_matrix as keys\n",
    "\n",
    "import os \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import rdkit, rdkit.Chem, rdkit.Chem.rdDepictor, rdkit.Chem.Draw\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r'/Users/suyashsachdeva/Desktop/GyanBhandar/toxic/'\n",
    "SMILE = os.listdir(PATH)\n",
    "smiles = []\n",
    "for smile in SMILE:\n",
    "    if smile[-6:]==\"smiles\":\n",
    "        smiles.append(smile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_elements = {6: \"C\", 8: \"O\", 1: \"H\", 17: \"Cl\", 7: \"N\", 9: \"F\", 16: \"S\",\n",
    "               35: \"Br\", 14: \"Si\", 11: \"Na\", 53: \"I\", 80: \"Hg\", 5: \"B\", \n",
    "               19: \"K\", 15: \"P\", 79: \"Au\", 24: \"Cr\", 50: \"Sn\", 20: \"Ca\",\n",
    "               48: \"Cd\", 30: \"Zn\", 23: \"V\", 33: \"As\", 3: \"Li\", 29: \"Cu\",\n",
    "               27:\"Co\", 47: \"Ag\", 34: \"Se\", 78: \"Pt\", 83: \"Bi\", 26: \"Fe\", \n",
    "               51: \"Sb\", 12: \"Mg\", 13: \"Al\", 81: \"Tl\", 56: \"Ba\", 22: \"V\", \n",
    "               40: \"Zr\", 38: \"Sr\", 28: \"Ni\", 49: \"In\", 32: \"Ge\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles2graph(sml):\n",
    "    \"\"\"Argument for the RD2NX function should be a valid SMILES sequence\n",
    "    returns: the graph\n",
    "    \"\"\"\n",
    "    m = rdkit.Chem.MolFromSmiles(sml)\n",
    "    m = rdkit.Chem.AddHs(m)\n",
    "    order_string = {\n",
    "        rdkit.Chem.rdchem.BondType.SINGLE: 1,\n",
    "        rdkit.Chem.rdchem.BondType.DOUBLE: 2,\n",
    "        rdkit.Chem.rdchem.BondType.TRIPLE: 3,\n",
    "        rdkit.Chem.rdchem.BondType.AROMATIC: 4,\n",
    "    }\n",
    "    N = len(list(m.GetAtoms()))\n",
    "    nodes = np.zeros((N, len(my_elements)))\n",
    "    lookup = list(my_elements.keys())\n",
    "    for i in m.GetAtoms():\n",
    "        nodes[i.GetIdx(), lookup.index(i.GetAtomicNum())] = 1\n",
    "\n",
    "    adj = np.zeros((N, N, 5))\n",
    "    for j in m.GetBonds():\n",
    "        u = min(j.GetBeginAtomIdx(), j.GetEndAtomIdx())\n",
    "        v = max(j.GetBeginAtomIdx(), j.GetEndAtomIdx())\n",
    "        order = j.GetBondType()\n",
    "        if order in order_string:\n",
    "            order = order_string[order]\n",
    "        else:\n",
    "            raise Warning(\"Ignoring bond order\" + order)\n",
    "        adj[u, v, order] = 1\n",
    "        adj[v, u, order] = 1\n",
    "    return nodes, adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:51:50] Explicit valence for atom # 2 Cl, 2, is greater than permitted\n",
      "[16:51:52] Explicit valence for atom # 3 Si, 8, is greater than permitted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2333 1098\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for smile in smiles[:1]:   \n",
    "    file = open(PATH+smile, 'r')\n",
    "    smile = file.read().split('\\n')\n",
    "    xtrain = []\n",
    "    ytrain = []\n",
    "    c=0\n",
    "    for x in smile:\n",
    "        if x!='':\n",
    "            x = x.split(\"\\t\")\n",
    "            if x[-1]=='1':\n",
    "                c = c+1\n",
    "    factor = int(len(smile)/float(c))-1\n",
    "    val1 = c\n",
    "    for c, x in enumerate(smile):\n",
    "        try:\n",
    "            x = x.split('\\t')\n",
    "            if x[-1]=='1' or c%factor==0:\n",
    "                nodes, adj= smiles2graph(x[0])\n",
    "                adj_mat = np.sum(adj, axis=-1) + np.eye(adj.shape[0])\n",
    "                degree = np.sum(adj_mat, axis=-1)\n",
    "                new_nodes = np.einsum(\"i,ij,jk->ik\", 1 / degree, adj_mat, nodes)\n",
    "                # print(new_nodes.shape, adj_mat.shape)\n",
    "                xtrain.append([Data(x=th.tensor(th.from_numpy(new_nodes), dtype=th.float32), edge_index=th.tensor(adj_mat, dtype=th.long).nonzero().t().contiguous()), th.from_numpy(np.array(x[2], dtype=\"float32\"))])\n",
    "        except:\n",
    "            pass\n",
    "    print(len(xtrain), val1)\n",
    "    dataset = DataLoader(xtrain, batch_size=1)\n",
    "    X.append(dataset)\n",
    "    Y.append(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAtt(pl.LightningModule):\n",
    "    def __init__(self, traindata, trainbatch=1, valbatch=1, lr=1e-4, num=5, indim=42, hidden_dim=64, dense = 256, factor=2):\n",
    "        super(GraphAtt, self).__init__()\n",
    "        self.gatconv = nn.ModuleList()\n",
    "        self.norm = nn.ModuleList()\n",
    "        for _ in range(num):\n",
    "            self.gatconv.append(gnn.GATConv(indim, hidden_dim, heads=1, dropout=0.2))\n",
    "            self.norm.append(gnn.BatchNorm(hidden_dim, momentum=0.99))\n",
    "            indim = hidden_dim\n",
    "            hidden_dim = hidden_dim*factor\n",
    "\n",
    "        self.act = nn.LeakyReLU(0.1)\n",
    "        self.pool = gnn.MeanAggregation()\n",
    "        self.dense = nn.Linear(hidden_dim//2, dense)\n",
    "        self.classify = nn.Linear(dense, 1)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "\n",
    "        self.traindata = traindata\n",
    "        self.trainbatch = trainbatch\n",
    "        self.valbatch = valbatch\n",
    "        self.lr = lr\n",
    "        self.loss = nn.BCELoss()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        for gat, norm in zip(self.gatconv, self.norm):\n",
    "            h = self.act(norm(gat(h, g)))\n",
    "        h = self.drop(self.pool(h))\n",
    "        out = F.sigmoid(self.classify(self.act(self.drop(self.dense(h)))))\n",
    "        return out\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.traindata\n",
    "\n",
    "    # def val_dataloader(self):\n",
    "    #     return DataLoader(self.valdata, batch_size=self.valbatch, shuffle=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr = 1e-4)\n",
    "        sch = optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size  = 10 , gamma = 0.1)\n",
    "        return {\n",
    "            \"optimizer\":optimizer,\n",
    "            \"lr_scheduler\" : {\n",
    "                \"scheduler\" : sch,\n",
    "                \"monitor\" : \"train_loss\",\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch[0]\n",
    "        y = batch[1]\n",
    "        h = x.x\n",
    "        g = th.tensor(x.edge_index, dtype=th.long)\n",
    "        y_pred = self.forward(g, h)\n",
    "        loss = self.loss(y_pred, y.reshape(-1, 1))\n",
    "        accuracy = self.get_accuracy(y.reshape(-1,1), y_pred)\n",
    "        self.log_dict({\"traning_loss\": loss, \"Accuracy\": accuracy}, on_step=True, on_epoch=True, prog_bar=True, logger=False)\n",
    "        return loss\n",
    "\n",
    "    # def validation_step(self, batch, batch_idx):\n",
    "    #     xtr, ytr = batch\n",
    "    #     h = xtr.x\n",
    "    #     g = th.tensor(xtr.edge_index, dtype=th.long)\n",
    "    #     y_pred = self.forward(g, h)\n",
    "    #     loss = self.loss(y_pred, ytr.reshape(-1,1))\n",
    "    #     accuracy = self.get_accuracy(ytr, y_pred)\n",
    "    #     self.log_dict({\"traning_loss\": loss, \"Accuracy\": accuracy}, on_step=True, on_epoch=True, prog_bar=True)\n",
    "    #     return loss\n",
    "\n",
    "    def get_accuracy(self, y_true, y_prob):\n",
    "        assert y_true.size() == y_prob.size()\n",
    "        y_prob = y_prob > 0.5\n",
    "        return (y_true == y_prob).sum().item() / y_true.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name     | Type            | Params\n",
      "---------------------------------------------\n",
      "0 | gatconv  | ModuleList      | 704 K \n",
      "1 | norm     | ModuleList      | 4.0 K \n",
      "2 | act      | LeakyReLU       | 0     \n",
      "3 | pool     | MeanAggregation | 0     \n",
      "4 | dense    | Linear          | 262 K \n",
      "5 | classify | Linear          | 257   \n",
      "6 | drop     | Dropout         | 0     \n",
      "7 | loss     | BCELoss         | 0     \n",
      "---------------------------------------------\n",
      "971 K     Trainable params\n",
      "0         Non-trainable params\n",
      "971 K     Total params\n",
      "3.886     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  10%|▉         | 224/2333 [00:06<01:00, 34.91it/s, loss=0.718, v_num=5, traning_loss_step=0.799, Accuracy_step=0.000]"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(min_epochs=10, max_epochs=30, enable_progress_bar=True)\n",
    "model = GraphAtt(traindata=dataset)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 3666349.65it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import trange\n",
    "for _ in trange(1000):\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 | packaged by conda-forge | (main, Oct 25 2022, 06:21:25) [Clang 14.0.4 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ee9af11aea572e4ec903c3dc598bbfb86677cb938927a901c90fbd17e22ef76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
