{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf \n",
    "# import tensorflow.keras.layers as layers\n",
    "# import tensorflow.keras.models as models\n",
    "# import tensorflow.keras.optimizers as optim\n",
    "\n",
    "# import spektral as sp \n",
    "# import spektral.layers as SL\n",
    "# import spektral.data.graph as graph\n",
    "# import spektral.data.loaders as loaders\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch_geometric as thg\n",
    "import torch_geometric.nn as gnn\n",
    "import torch_geometric.nn.functional as gf\n",
    "from torch_geometric.data import Data, HeteroData\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils.convert import from_scipy_sparse_matrix as keys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rdkit, rdkit.Chem, rdkit.Chem.rdDepictor, rdkit.Chem.Draw\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "DENSE = r\"C:\\Users\\suyash\\Desktop\\toxic\\/tox21_dense_train.csv\"\n",
    "LABEL = r\"C:\\Users\\suyash\\Desktop\\toxic\\tox21_labels_train.csv\"\n",
    "SMILE = r\"C:\\Users\\suyash\\Desktop\\toxic\\nr-ar.smiles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_elements = {6: \"C\", 8: \"O\", 1: \"H\", 17: \"Cl\", 7: \"N\", 9: \"F\", 16: \"S\",\n",
    "               35: \"Br\", 14: \"Si\", 11: \"Na\", 53: \"I\", 80: \"Hg\", 5: \"B\", \n",
    "               19: \"K\", 15: \"P\", 79: \"Au\", 24: \"Cr\", 50: \"Sn\", 20: \"Ca\",\n",
    "               48: \"Cd\", 30: \"Zn\", 23: \"V\", 33: \"As\", 3: \"Li\", 29: \"Cu\",\n",
    "               27:\"Co\", 47: \"Ag\", 34: \"Se\", 78: \"Pt\", 83: \"Bi\", 26: \"Fe\", \n",
    "               51: \"Sb\", 12: \"Mg\", 13: \"Al\", 81: \"Tl\", 56: \"Ba\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles2graph(sml):\n",
    "    \"\"\"Argument for the RD2NX function should be a valid SMILES sequence\n",
    "    returns: the graph\n",
    "    \"\"\"\n",
    "    m = rdkit.Chem.MolFromSmiles(sml)\n",
    "    m = rdkit.Chem.AddHs(m)\n",
    "    order_string = {\n",
    "        rdkit.Chem.rdchem.BondType.SINGLE: 1,\n",
    "        rdkit.Chem.rdchem.BondType.DOUBLE: 2,\n",
    "        rdkit.Chem.rdchem.BondType.TRIPLE: 3,\n",
    "        rdkit.Chem.rdchem.BondType.AROMATIC: 4,\n",
    "    }\n",
    "    N = len(list(m.GetAtoms()))\n",
    "    nodes = np.zeros((N, len(my_elements)))\n",
    "    lookup = list(my_elements.keys())\n",
    "    for i in m.GetAtoms():\n",
    "        nodes[i.GetIdx(), lookup.index(i.GetAtomicNum())] = 1\n",
    "\n",
    "    adj = np.zeros((N, N, 5))\n",
    "    for j in m.GetBonds():\n",
    "        u = min(j.GetBeginAtomIdx(), j.GetEndAtomIdx())\n",
    "        v = max(j.GetBeginAtomIdx(), j.GetEndAtomIdx())\n",
    "        order = j.GetBondType()\n",
    "        if order in order_string:\n",
    "            order = order_string[order]\n",
    "        else:\n",
    "            raise Warning(\"Ignoring bond order\" + order)\n",
    "        adj[u, v, order] = 1\n",
    "        adj[v, u, order] = 1\n",
    "    return nodes, adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suyash\\AppData\\Local\\Temp\\ipykernel_21252\\4243487382.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xtrain.append(Data(x=th.tensor(th.from_numpy(new_nodes), dtype=th.float32), edge_index=th.tensor(adj_mat, dtype=th.long).nonzero().t().contiguous()))\n",
      "[09:12:48] Explicit valence for atom # 3 Si, 8, is greater than permitted\n"
     ]
    }
   ],
   "source": [
    "file = open(SMILE, 'r')\n",
    "smile = file.read().split('\\n')\n",
    "xtrain = []\n",
    "ytrain = []\n",
    "\n",
    "for c, x in enumerate(smile):\n",
    "    try:\n",
    "        x = x.split('\\t')\n",
    "        if x[2]=='1' or c%20==0:\n",
    "            nodes, adj= smiles2graph(x[0])\n",
    "            adj_mat = np.sum(adj, axis=-1) + np.eye(adj.shape[0])\n",
    "            degree = np.sum(adj_mat, axis=-1)\n",
    "            new_nodes = np.einsum(\"i,ij,jk->ik\", 1 / degree, adj_mat, nodes)\n",
    "            # print(new_nodes.shape, adj_mat.shape)\n",
    "            xtrain.append(Data(x=th.tensor(th.from_numpy(new_nodes), dtype=th.float32), edge_index=th.tensor(adj_mat, dtype=th.long).nonzero().t().contiguous()))\n",
    "            ytrain.append(x[2])\n",
    "    except:\n",
    "        pass\n",
    "ytrain = th.from_numpy(np.array(ytrain, dtype=\"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "822"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [x for x in DataLoader(xtrain, batch_size=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(models.Model):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.mask = SL.GraphMasking()\n",
    "#         self.conv1 = SL.GCSConv(32, activation=\"relu\")\n",
    "#         # self.pool = SL.MinCutPool(N // 2)\n",
    "#         self.conv2 = SL.GCSConv(16, activation=\"relu\")\n",
    "#         self.conv3 = SL.GCSConv(8, activation=\"relu\")\n",
    "#         self.global_pool = SL.GlobalSumPool()\n",
    "#         self.dense1 = layers.Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         x, a = inputs\n",
    "#         x = self.mask(x)\n",
    "#         x = self.conv1([x, a])\n",
    "#         # x_pool, a_pool = self.pool([x, a])\n",
    "#         x_pool = self.conv2([x, a])\n",
    "#         output = self.global_pool(x_pool)\n",
    "#         output = self.dense1(output)\n",
    "\n",
    "#         return output\n",
    "\n",
    "# learning_rate=1e-4\n",
    "# model = Net()\n",
    "# opt = optim.Adam(lr=learning_rate)\n",
    "# model.compile(optimizer=opt, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Should work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim=36, hidden_dim=64):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = gnn.GraphConv(in_dim, hidden_dim)\n",
    "        self.conv2 = gnn.GraphConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = gnn.GraphConv(hidden_dim, hidden_dim*2)\n",
    "        self.conv4 = gnn.GraphConv(hidden_dim*2, hidden_dim*2)\n",
    "        self.pool  = gnn.MeanAggregation()\n",
    "        self.dense = nn.Linear(hidden_dim*2, hidden_dim)\n",
    "        self.classify = nn.Linear(hidden_dim,1)\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        # Apply graph convolution and activation.\n",
    "        h = F.relu(self.conv1(h, g))\n",
    "        h = F.relu(self.conv2(h, g))\n",
    "        h = F.relu(self.conv3(h, g))\n",
    "        h = F.relu(self.conv4(h, g))\n",
    "        h = self.pool(h)\n",
    "        h = F.relu(self.dense(h))\n",
    "        return F.sigmoid(self.classify(h))\n",
    "        \n",
    "\n",
    "model = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_true, y_prob):\n",
    "    assert y_true.size() == y_prob.size()\n",
    "    y_prob = y_prob > 0.5\n",
    "    return (y_true == y_prob).sum().item() / y_true.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suyash\\AppData\\Local\\Temp\\ipykernel_21252\\2871627940.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  g = th.tensor(xtr.edge_index, dtype=th.long)\n",
      "c:\\Users\\suyash\\Desktop\\KACHRA\\laohub\\Smile_in_Pain\\Ajgar_Ke_Jalve\\Artificial_Intelligence\\Neural_Networks\\Supervised_Learning\\Graphic_Nets\\Project\\graph\\lib\\site-packages\\torch\\nn\\functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 || Loss: 0.5818 || Accuracy: 0.6740 \n",
      "\n",
      "Epoch: 2/100 || Loss: 0.5782 || Accuracy: 0.7044 \n",
      "\n",
      "Epoch: 3/100 || Loss: 0.5234 || Accuracy: 0.7494 \n",
      "\n",
      "Epoch: 4/100 || Loss: 0.5043 || Accuracy: 0.7530 \n",
      "\n",
      "Epoch: 5/100 || Loss: 0.4929 || Accuracy: 0.7652 \n",
      "\n",
      "Epoch: 6/100 || Loss: 0.4822 || Accuracy: 0.7652 \n",
      "\n",
      "Epoch: 7/100 || Loss: 0.4753 || Accuracy: 0.7725 \n",
      "\n",
      "Epoch: 8/100 || Loss: 0.4661 || Accuracy: 0.7725 \n",
      "\n",
      "Epoch: 9/100 || Loss: 0.4612 || Accuracy: 0.7835 \n",
      "\n",
      "Epoch: 10/100 || Loss: 0.4560 || Accuracy: 0.7798 \n",
      "\n",
      "Epoch: 11/100 || Loss: 0.4523 || Accuracy: 0.7822 \n",
      "\n",
      "Epoch: 12/100 || Loss: 0.4504 || Accuracy: 0.7835 \n",
      "\n",
      "Epoch: 13/100 || Loss: 0.4480 || Accuracy: 0.7883 \n",
      "\n",
      "Epoch: 14/100 || Loss: 0.4458 || Accuracy: 0.7871 \n",
      "\n",
      "Epoch: 15/100 || Loss: 0.4438 || Accuracy: 0.7871 \n",
      "\n",
      "Epoch: 16/100 || Loss: 0.4428 || Accuracy: 0.7859 \n",
      "\n",
      "Epoch: 17/100 || Loss: 0.4413 || Accuracy: 0.7859 \n",
      "\n",
      "Epoch: 18/100 || Loss: 0.4399 || Accuracy: 0.7847 \n",
      "\n",
      "Epoch: 19/100 || Loss: 0.4393 || Accuracy: 0.7835 \n",
      "\n",
      "Epoch: 20/100 || Loss: 0.4389 || Accuracy: 0.7822 \n",
      "\n",
      "Epoch: 21/100 || Loss: 0.4388 || Accuracy: 0.7810 \n",
      "\n",
      "Epoch: 22/100 || Loss: 0.4387 || Accuracy: 0.7822 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [243], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m batch \u001b[39m=\u001b[39m xtr\u001b[39m.\u001b[39mbatch\n\u001b[0;32m     14\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 15\u001b[0m ypred \u001b[39m=\u001b[39m model(g,h)\n\u001b[0;32m     16\u001b[0m loss \u001b[39m=\u001b[39m criterian(ypred, ytr\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m))\n\u001b[0;32m     18\u001b[0m \u001b[39mif\u001b[39;00m ypred\u001b[39m>\u001b[39m\u001b[39m0.5\u001b[39m \u001b[39mand\u001b[39;00m ytr\u001b[39m>\u001b[39m\u001b[39m0.5\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\suyash\\Desktop\\KACHRA\\laohub\\Smile_in_Pain\\Ajgar_Ke_Jalve\\Artificial_Intelligence\\Neural_Networks\\Supervised_Learning\\Graphic_Nets\\Project\\graph\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [241], line 14\u001b[0m, in \u001b[0;36mClassifier.forward\u001b[1;34m(self, g, h)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, g, h):\n\u001b[0;32m     13\u001b[0m     \u001b[39m# Apply graph convolution and activation.\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m     h \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(h, g))\n\u001b[0;32m     15\u001b[0m     h \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(h, g))\n\u001b[0;32m     16\u001b[0m     h \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(h, g))\n",
      "File \u001b[1;32mc:\\Users\\suyash\\Desktop\\KACHRA\\laohub\\Smile_in_Pain\\Ajgar_Ke_Jalve\\Artificial_Intelligence\\Neural_Networks\\Supervised_Learning\\Graphic_Nets\\Project\\graph\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\suyash\\Desktop\\KACHRA\\laohub\\Smile_in_Pain\\Ajgar_Ke_Jalve\\Artificial_Intelligence\\Neural_Networks\\Supervised_Learning\\Graphic_Nets\\Project\\graph\\lib\\site-packages\\torch_geometric\\nn\\conv\\graph_conv.py:79\u001b[0m, in \u001b[0;36mGraphConv.forward\u001b[1;34m(self, x, edge_index, edge_weight, size)\u001b[0m\n\u001b[0;32m     76\u001b[0m     x: OptPairTensor \u001b[39m=\u001b[39m (x, x)\n\u001b[0;32m     78\u001b[0m \u001b[39m# propagate_type: (x: OptPairTensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate(edge_index, x\u001b[39m=\u001b[39;49mx, edge_weight\u001b[39m=\u001b[39;49medge_weight,\n\u001b[0;32m     80\u001b[0m                      size\u001b[39m=\u001b[39;49msize)\n\u001b[0;32m     81\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin_rel(out)\n\u001b[0;32m     83\u001b[0m x_r \u001b[39m=\u001b[39m x[\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\suyash\\Desktop\\KACHRA\\laohub\\Smile_in_Pain\\Ajgar_Ke_Jalve\\Artificial_Intelligence\\Neural_Networks\\Supervised_Learning\\Graphic_Nets\\Project\\graph\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:391\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[1;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    389\u001b[0m         aggr_kwargs \u001b[39m=\u001b[39m res[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(res, \u001b[39mtuple\u001b[39m) \u001b[39melse\u001b[39;00m res\n\u001b[1;32m--> 391\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maggregate(out, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39maggr_kwargs)\n\u001b[0;32m    393\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_aggregate_forward_hooks\u001b[39m.\u001b[39mvalues():\n\u001b[0;32m    394\u001b[0m     res \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, (aggr_kwargs, ), out)\n",
      "File \u001b[1;32mc:\\Users\\suyash\\Desktop\\KACHRA\\laohub\\Smile_in_Pain\\Ajgar_Ke_Jalve\\Artificial_Intelligence\\Neural_Networks\\Supervised_Learning\\Graphic_Nets\\Project\\graph\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:514\u001b[0m, in \u001b[0;36mMessagePassing.aggregate\u001b[1;34m(self, inputs, index, ptr, dim_size)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maggregate\u001b[39m(\u001b[39mself\u001b[39m, inputs: Tensor, index: Tensor,\n\u001b[0;32m    502\u001b[0m               ptr: Optional[Tensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    503\u001b[0m               dim_size: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    504\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Aggregates messages from neighbors as\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[39m    :math:`\\square_{j \\in \\mathcal{N}(i)}`.\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39m    as specified in :meth:`__init__` by the :obj:`aggr` argument.\u001b[39;00m\n\u001b[0;32m    513\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 514\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maggr_module(inputs, index, ptr\u001b[39m=\u001b[39;49mptr, dim_size\u001b[39m=\u001b[39;49mdim_size,\n\u001b[0;32m    515\u001b[0m                             dim\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_dim)\n",
      "File \u001b[1;32mc:\\Users\\suyash\\Desktop\\KACHRA\\laohub\\Smile_in_Pain\\Ajgar_Ke_Jalve\\Artificial_Intelligence\\Neural_Networks\\Supervised_Learning\\Graphic_Nets\\Project\\graph\\lib\\site-packages\\torch_geometric\\nn\\aggr\\base.py:109\u001b[0m, in \u001b[0;36mAggregation.__call__\u001b[1;34m(self, x, index, ptr, dim_size, dim, **kwargs)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[39mif\u001b[39;00m dim_size \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    108\u001b[0m         dim_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(index\u001b[39m.\u001b[39mmax()) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m index\u001b[39m.\u001b[39mnumel() \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m--> 109\u001b[0m     \u001b[39melif\u001b[39;00m index\u001b[39m.\u001b[39mnumel() \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m dim_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(index\u001b[39m.\u001b[39;49mmax()):\n\u001b[0;32m    110\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEncountered invalid \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdim_size\u001b[39m\u001b[39m'\u001b[39m\u001b[39m (got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m                          \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdim_size\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m but expected \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    112\u001b[0m                          \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m>= \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mint\u001b[39m(index\u001b[39m.\u001b[39mmax()) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    114\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(x, index, ptr, dim_size, dim, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "criterian = nn.BCELoss()\n",
    "learning_rate = 1e-4\n",
    "decay = 1e-2\n",
    "for epoch in range(epochs):\n",
    "    lss = 0\n",
    "    acc = 0\n",
    "    learning_rate = learning_rate/(epoch*decay+1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for step, (xtr, ytr) in enumerate(zip(dataset, ytrain)):\n",
    "        h = xtr.x\n",
    "        g = th.tensor(xtr.edge_index, dtype=th.long)\n",
    "        batch = xtr.batch\n",
    "        optimizer.zero_grad()\n",
    "        ypred = model(g,h)\n",
    "        loss = criterian(ypred, ytr.reshape(-1,1))\n",
    "\n",
    "        if ypred>0.5 and ytr>0.5:\n",
    "            acc=acc+1\n",
    "        elif ypred<=0.5 and ytr<=0.5:\n",
    "            acc=acc+1\n",
    "        # print(ypred, ytr)\n",
    "        lss = lss+loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}/{epochs} || Loss: {lss/(step+1):.4f} || Accuracy: {acc/(step+1):.4f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0128]], grad_fn=<SigmoidBackward0>), tensor(0.))"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred, ytr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 ('graph': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e067a7d41aaecea2c3cb96f2320fd4300af65f2300805fdda9e01e34a6b5804"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
